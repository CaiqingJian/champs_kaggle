{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T21:33:37.486688Z",
     "start_time": "2019-08-28T21:33:37.483369Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T21:33:37.879413Z",
     "start_time": "2019-08-28T21:33:37.876979Z"
    }
   },
   "outputs": [],
   "source": [
    "model_B = [\n",
    "    '9ZB-000-link-edges.660000',\n",
    "    '9ZB-000-link-edges.560000',\n",
    "    '9ZB-000-link-edges.460000',\n",
    "    '9ZB2-000-link-edges-4xlowerlr.660000',\n",
    "    '9ZB2-000-link-edges-4xlowerlr.560000',\n",
    "    '9ZB2-000-link-edges-4xlowerlr.460000',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T21:33:38.057314Z",
     "start_time": "2019-08-28T21:33:38.054295Z"
    }
   },
   "outputs": [],
   "source": [
    "model_F = [\n",
    "    '9ZF-000-ablation-study-high-batch-size-2.660000',\n",
    "    '9ZF-000-ablation-study-high-batch-size-2.560000',\n",
    "    '9ZF-000-ablation-study-high-batch-size-2.460000',\n",
    "]\n",
    "\n",
    "\n",
    "model_F2 = [\n",
    "    '9ZF2-000-ablation-study-high-batch-size-augment-really-this-time.206250',\n",
    "    '9ZF2-000-ablation-study-high-batch-size-augment-really-this-time.176250',\n",
    "    '9ZF2-000-ablation-study-high-batch-size-augment-really-this-time.146250',\n",
    "]    \n",
    "\n",
    "model_F3 = [\n",
    "    '9ZF3-000-ablation-study-remove-global-state.206250',\n",
    "    '9ZF3-000-ablation-study-remove-global-state.176250',\n",
    "    '9ZF3-000-ablation-study-remove-global-state.146250',\n",
    "]\n",
    "\n",
    "model_F5 = [\n",
    "    '9ZF5-000-ablation-study-no-pairs-embeddings-and-one-preprocessing-edge-pairs.206250',\n",
    "    '9ZF5-000-ablation-study-no-pairs-embeddings-and-one-preprocessing-edge-pairs.176250',\n",
    "    '9ZF5-000-ablation-study-no-pairs-embeddings-and-one-preprocessing-edge-pairs.146250',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T21:33:38.282725Z",
     "start_time": "2019-08-28T21:33:38.280389Z"
    }
   },
   "outputs": [],
   "source": [
    "model_G = [    \n",
    "    '9ZG-000-vanilla-deep-radam.3300000',\n",
    "    '9ZG-000-vanilla-deep-radam.3000000',\n",
    "    '9ZG-000-vanilla-deep-radam.2700000'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T21:33:38.465538Z",
     "start_time": "2019-08-28T21:33:38.463200Z"
    }
   },
   "outputs": [],
   "source": [
    "model_G2 = [\n",
    "    '9ZG2-000-vanilla-deep-radam-300-10.1006500',\n",
    "    '9ZG2-000-vanilla-deep-radam-300-10.856500',\n",
    "    '9ZG2-000-vanilla-deep-radam-300-10.706500',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T21:33:38.647160Z",
     "start_time": "2019-08-28T21:33:38.644478Z"
    }
   },
   "outputs": [],
   "source": [
    "model_G4 = [\n",
    "    '9ZG4-000-vanilla-deep-radam.660000',\n",
    "    '9ZG4-000-vanilla-deep-radam.560000',\n",
    "    '9ZG4-000-vanilla-deep-radam.460000'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T21:33:38.816333Z",
     "start_time": "2019-08-28T21:33:38.813970Z"
    }
   },
   "outputs": [],
   "source": [
    "model_G5 = [\n",
    "    '9ZG5-000-vanilla-deep-radam-mse-clip.660000',\n",
    "    '9ZG5-000-vanilla-deep-radam-mse-clip.560000',\n",
    "    '9ZG5-000-vanilla-deep-radam-mse-clip.460000',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T21:33:38.989896Z",
     "start_time": "2019-08-28T21:33:38.987677Z"
    }
   },
   "outputs": [],
   "source": [
    "model_I = [    \n",
    "    '9ZI-007-distributionnal-loss.660000',\n",
    "    '9ZI-005-distributionnal-loss.560000',\n",
    "    '9ZI-005-distributionnal-loss.460000',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit / valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T21:33:40.075478Z",
     "start_time": "2019-08-28T21:33:40.071191Z"
    }
   },
   "outputs": [],
   "source": [
    "from common import *\n",
    "from dataset.dataset_9ZB_117_edge_link import EdgeBasedDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.data import Batch\n",
    "from tensorboardX import SummaryWriter\n",
    "from scheduler_superconvergence_09J import *\n",
    "from torch_geometric.data import DataListLoader\n",
    "from torch_scatter import scatter_add\n",
    "from importancer import get_tags, select_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T21:33:40.306597Z",
     "start_time": "2019-08-28T21:33:40.287656Z"
    }
   },
   "outputs": [],
   "source": [
    "def init_dataset():\n",
    "    global train_loader\n",
    "    global train_small_loader\n",
    "    global valid_loader\n",
    "    global train_indices\n",
    "    global valid_indices\n",
    "    \n",
    "    global submit_loader\n",
    "    \n",
    "    if action == 'train':\n",
    "        if to_load:\n",
    "            train_indices = to_load['train_indices']\n",
    "            valid_indices = to_load['valid_indices']\n",
    "        else:\n",
    "            indices = list(range(len(dataset)))\n",
    "            train_indices, valid_indices = train_test_split(indices, test_size = 5000, random_state = 1234)\n",
    "            \n",
    "        train_big_indices, train_small_indices = train_test_split(list(range(len(train_indices))), test_size = 5000, random_state = 1234)\n",
    "\n",
    "        train = torch.utils.data.Subset(dataset, train_indices)\n",
    "        train_small = torch.utils.data.Subset(train, train_small_indices)\n",
    "        valid = torch.utils.data.Subset(dataset, valid_indices)\n",
    "\n",
    "        if not parallel_gpu:\n",
    "            train_loader = DataLoader(train, batch_size = batch_size, drop_last = True, shuffle = True, follow_batch=['edge_attr_numeric'], num_workers=num_workers)\n",
    "            train_small_loader = DataLoader(train_small, batch_size = batch_size * valid_batch_size_factor, drop_last = True, shuffle = True, follow_batch=['edge_attr_numeric'], num_workers=num_workers)\n",
    "            valid_loader = DataLoader(valid, batch_size = batch_size * valid_batch_size_factor, drop_last = True, shuffle = True, follow_batch=['edge_attr_numeric'], num_workers=num_workers)\n",
    "        else:\n",
    "            train_loader = DataListLoader(train, batch_size = batch_size, shuffle = True, num_workers=num_workers)\n",
    "            valid_loader = DataListLoader(valid, batch_size = batch_size * valid_batch_size_factor, shuffle = True, num_workers=num_workers)\n",
    "\n",
    "        if False and \"benchmark\":\n",
    "            for batch in tqdm.tqdm_notebook(train_loader):\n",
    "                pass\n",
    "    else:\n",
    "        if not parallel_gpu:\n",
    "            submit_loader = DataLoader(dataset, batch_size = batch_size * valid_batch_size_factor, drop_last = False, shuffle = False, follow_batch=['edge_attr_numeric'], num_workers=num_workers)\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        if False and \"benchmark\":\n",
    "            for batch in tqdm.tqdm_notebook(submit_loader):\n",
    "                pass\n",
    "            \n",
    "def init_model():\n",
    "    global model\n",
    "    global MEGNetList\n",
    "    \n",
    "    model = MEGNetList(\n",
    "        layer_count,                \n",
    "        atom_embedding_count, bond_ebedding_count, global_embedding_count, \n",
    "        atom_input_size, bond_input_size, global_input_size, \n",
    "        hidden, \n",
    "        target_means, target_stds)\n",
    "    \n",
    "    if to_load:\n",
    "        model.load_state_dict(to_load['model'])\n",
    "\n",
    "    if not parallel_gpu:\n",
    "        model = model.to(device)\n",
    "    else:\n",
    "        model = model.to('cuda:0')\n",
    "        \n",
    "        \n",
    "def batch_submit():\n",
    "    global batch\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # BATCH\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # PREDICT\n",
    "        out = model.forward(\n",
    "            [batch.x_numeric],\n",
    "            batch.x_embeddings,\n",
    "\n",
    "            [batch.edge_attr_numeric], \n",
    "            batch.edge_attr_embeddings,\n",
    "\n",
    "            [batch.u_numeric],\n",
    "            batch.u_embeddings,\n",
    "\n",
    "            batch.edge_index, \n",
    "\n",
    "            batch.batch, \n",
    "            batch.edge_attr_numeric_batch, \n",
    "\n",
    "            batch.y_types, \n",
    "\n",
    "            batch.cycles_edge_index,\n",
    "            batch.cycles_id,\n",
    "\n",
    "            batch.edges_connectivity_ids,\n",
    "            batch.edges_connectivity_features,\n",
    "        )\n",
    "        \n",
    "        return out.squeeze(1).cpu().numpy()\n",
    "\n",
    "def submit(loader):\n",
    "    global batch\n",
    "    model.eval()\n",
    "    \n",
    "    predictions = dataset.bond_descriptors.reset_index()[['type', 'edge_index', 'atom_index_0', 'atom_index_1', 'molecule_id']]\n",
    "    predictions = predictions.loc[predictions['molecule_id'].isin(dataset.molecules_ids)]\n",
    "\n",
    "    predictions['prediction'] = np.nan\n",
    "\n",
    "    molecule_map = {k : v for k, v in zip(dataset.molecules_ids, dataset.molecules)}\n",
    "\n",
    "    predictions['molecule_name'] = predictions['molecule_id'].map(molecule_map)\n",
    "\n",
    "    predictions = predictions.set_index('molecule_id')\n",
    "\n",
    "    current_index = 0\n",
    "    for batch in tqdm.tqdm_notebook(loader):\n",
    "        try:\n",
    "            molecule_ids = batch.molecule_ids.numpy()\n",
    "\n",
    "            prediction = batch_submit()\n",
    "\n",
    "            predictions.loc[molecule_ids, 'prediction'] = prediction\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Escaping\")\n",
    "            return \"escape\"\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T21:33:40.498556Z",
     "start_time": "2019-08-28T21:33:40.472434Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_submit_dataset():\n",
    "    global dataset\n",
    "    \n",
    "    global global_embedding_count\n",
    "    global atom_embedding_count\n",
    "    global bond_ebedding_count\n",
    "    global global_numeric_count\n",
    "    global bond_numeric_count\n",
    "    global atom_numeric_count\n",
    "    global target_means\n",
    "    global target_stds\n",
    "    global atom_input_size\n",
    "    global bond_input_size\n",
    "    global global_input_size\n",
    "    global action\n",
    "    \n",
    "    \n",
    "    action = 'submit'\n",
    "    submit_dataset_name = 'test'\n",
    "\n",
    "    if action == 'train':\n",
    "        dataset = EdgeBasedDataset(name = 'train')\n",
    "    else:\n",
    "        dataset = EdgeBasedDataset(name = submit_dataset_name)\n",
    "\n",
    "    target_stats = dataset.bond_descriptors.loc[(dataset.bond_descriptors['type'] != 'VOID') & dataset.bond_descriptors.index.isin(dataset.dataset.loc[dataset.dataset['dataset'] == 'train', 'molecule_id'])].groupby('type_id')['scalar_coupling_constant'].agg(['std', 'median'])\n",
    "\n",
    "    target_means = target_stats['median'].values\n",
    "    target_stds = target_stats['std'].values\n",
    "    target_stats\n",
    "\n",
    "    # Inputs\n",
    "\n",
    "    sample = dataset[0]\n",
    "    print(sample)\n",
    "\n",
    "    global_embedding_count = dataset.global_embedding_count\n",
    "    atom_embedding_count = dataset.atom_embedding_count\n",
    "    bond_ebedding_count = dataset.bond_ebedding_count\n",
    "\n",
    "    global_numeric_count = sample.u_numeric.size(1)\n",
    "    bond_numeric_count = sample.edge_attr_numeric.size(1)\n",
    "    atom_numeric_count = sample.x_numeric.size(1)\n",
    "    \n",
    "    atom_input_size = [(atom_numeric_count, hidden)]\n",
    "    bond_input_size = [(bond_numeric_count, hidden)]\n",
    "    global_input_size = [(global_numeric_count, hidden)]\n",
    "    \n",
    "def load_train_dataset():\n",
    "    global dataset\n",
    "    \n",
    "    global global_embedding_count\n",
    "    global atom_embedding_count\n",
    "    global bond_ebedding_count\n",
    "    global global_numeric_count\n",
    "    global bond_numeric_count\n",
    "    global atom_numeric_count\n",
    "    global target_means\n",
    "    global target_stds\n",
    "    global atom_input_size\n",
    "    global bond_input_size\n",
    "    global global_input_size\n",
    "    global action\n",
    "    \n",
    "    action = 'submit'\n",
    "    submit_dataset_name = 'train'\n",
    "\n",
    "    if action == 'train':\n",
    "        dataset = EdgeBasedDataset(name = 'train')\n",
    "    else:\n",
    "        dataset = EdgeBasedDataset(name = submit_dataset_name)\n",
    "\n",
    "    target_stats = dataset.bond_descriptors.loc[(dataset.bond_descriptors['type'] != 'VOID') & dataset.bond_descriptors.index.isin(dataset.dataset.loc[dataset.dataset['dataset'] == 'train', 'molecule_id'])].groupby('type_id')['scalar_coupling_constant'].agg(['std', 'median'])\n",
    "\n",
    "    target_means = target_stats['median'].values\n",
    "    target_stds = target_stats['std'].values\n",
    "    target_stats\n",
    "\n",
    "    # Inputs\n",
    "\n",
    "    sample = dataset[0]\n",
    "    print(sample)\n",
    "\n",
    "    global_embedding_count = dataset.global_embedding_count\n",
    "    atom_embedding_count = dataset.atom_embedding_count\n",
    "    bond_ebedding_count = dataset.bond_ebedding_count\n",
    "\n",
    "    global_numeric_count = sample.u_numeric.size(1)\n",
    "    bond_numeric_count = sample.edge_attr_numeric.size(1)\n",
    "    atom_numeric_count = sample.x_numeric.size(1)\n",
    "    \n",
    "    atom_input_size = [(atom_numeric_count, hidden)]\n",
    "    bond_input_size = [(bond_numeric_count, hidden)]\n",
    "    global_input_size = [(global_numeric_count, hidden)]\n",
    "    \n",
    "    \n",
    "def package_submit_prediction(submit_predictions):\n",
    "    assert submit_predictions.loc[submit_predictions['prediction'].notnull()].shape[0] == 7223027\n",
    "\n",
    "    test = pd.read_csv('data/test.csv')\n",
    "\n",
    "    test['prediction'] = np.nan\n",
    "\n",
    "    pred_1 = pd.merge(test[['molecule_name', 'atom_index_0', 'atom_index_1']], submit_predictions[['molecule_name', 'atom_index_0', 'atom_index_1', 'prediction']], left_on = ['molecule_name', 'atom_index_0', 'atom_index_1'], right_on = ['molecule_name', 'atom_index_0', 'atom_index_1'], how = 'left')['prediction']\n",
    "    pred_2 = pd.merge(test[['molecule_name', 'atom_index_0', 'atom_index_1']], submit_predictions[['molecule_name', 'atom_index_0', 'atom_index_1', 'prediction']], left_on = ['molecule_name', 'atom_index_0', 'atom_index_1'], right_on = ['molecule_name', 'atom_index_1', 'atom_index_0'], how = 'left')['prediction']\n",
    "\n",
    "    test['prediction'] = pred_1\n",
    "    test.loc[test['prediction'].isnull(), 'prediction'] = pred_2\n",
    "\n",
    "    test = test[['id', 'prediction']].rename(columns = {'prediction' : 'scalar_coupling_constant'})\n",
    "    test = test.rename(columns = {'prediction' : 'scalar_coupling_constant'})\n",
    "\n",
    "    assert test['scalar_coupling_constant'].isnull().sum() == 0\n",
    "    assert test['scalar_coupling_constant'].notnull().sum() == 2505542\n",
    "\n",
    "    return test\n",
    "\n",
    "def package_train_prediction(train_predictions):\n",
    "    assert train_predictions.loc[train_predictions['prediction'].notnull()].shape[0] == 13432555\n",
    "\n",
    "    train = pd.read_csv('data/train.csv')\n",
    "\n",
    "    train['prediction'] = np.nan\n",
    "\n",
    "    pred_1 = pd.merge(train[['molecule_name', 'atom_index_0', 'atom_index_1']], train_predictions[['molecule_name', 'atom_index_0', 'atom_index_1', 'prediction']], left_on = ['molecule_name', 'atom_index_0', 'atom_index_1'], right_on = ['molecule_name', 'atom_index_0', 'atom_index_1'], how = 'left')['prediction']\n",
    "    pred_2 = pd.merge(train[['molecule_name', 'atom_index_0', 'atom_index_1']], train_predictions[['molecule_name', 'atom_index_0', 'atom_index_1', 'prediction']], left_on = ['molecule_name', 'atom_index_0', 'atom_index_1'], right_on = ['molecule_name', 'atom_index_1', 'atom_index_0'], how = 'left')['prediction']\n",
    "\n",
    "    train['prediction'] = pred_1\n",
    "    train.loc[train['prediction'].isnull(), 'prediction'] = pred_2\n",
    "\n",
    "    assert train['prediction'].isnull().sum() == 0\n",
    "    assert train['prediction'].notnull().sum() == 4658147\n",
    "\n",
    "    train['dataset'] = 'train'\n",
    "\n",
    "    indices = list(range(len(dataset)))\n",
    "    train_indices, valid_indices = train_test_split(indices, test_size = 5000, random_state = 1234)\n",
    "    valid_molecules = [dataset.molecules[i] for i in valid_indices]\n",
    "\n",
    "    train.loc[train['molecule_name'].isin(valid_molecules), 'dataset'] = 'valid'\n",
    "    assert train.loc[train['dataset'] == 'valid', 'molecule_name'].nunique() == 5000\n",
    "\n",
    "    return train\n",
    "\n",
    "def analyze(train):\n",
    "    train = train.loc[train['dataset'] == 'valid']\n",
    "    train['mae'] = (train['prediction'] - train['scalar_coupling_constant']).abs()\n",
    "    \n",
    "    print(np.log(train.groupby('type')['mae'].mean()).mean())\n",
    "    #print(np.log(train.groupby('type')['mae'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T21:33:40.631946Z",
     "start_time": "2019-08-28T21:33:40.619755Z"
    }
   },
   "outputs": [],
   "source": [
    "def init_valid_dataset():\n",
    "    global train_loader\n",
    "    global train_small_loader\n",
    "    global valid_loader\n",
    "    global train_indices\n",
    "    global valid_indices\n",
    "    \n",
    "    global submit_loader\n",
    "    \n",
    "    if action == 'train':\n",
    "        if to_load:\n",
    "            train_indices = to_load['train_indices']\n",
    "            valid_indices = to_load['valid_indices']\n",
    "        else:\n",
    "            indices = list(range(len(dataset)))\n",
    "            train_indices, valid_indices = train_test_split(indices, test_size = 5000, random_state = 1234)\n",
    "            \n",
    "        train_big_indices, train_small_indices = train_test_split(list(range(len(train_indices))), test_size = 5000, random_state = 1234)\n",
    "\n",
    "        train = torch.utils.data.Subset(dataset, train_indices)\n",
    "        train_small = torch.utils.data.Subset(train, train_small_indices)\n",
    "        valid = torch.utils.data.Subset(dataset, valid_indices)\n",
    "\n",
    "        if not parallel_gpu:\n",
    "            train_loader = DataLoader(train, batch_size = batch_size, drop_last = True, shuffle = True, follow_batch=['edge_attr_numeric'], num_workers=num_workers)\n",
    "            train_small_loader = DataLoader(train_small, batch_size = batch_size * valid_batch_size_factor, drop_last = True, shuffle = True, follow_batch=['edge_attr_numeric'], num_workers=num_workers)\n",
    "            valid_loader = DataLoader(valid, batch_size = batch_size * valid_batch_size_factor, drop_last = True, shuffle = True, follow_batch=['edge_attr_numeric'], num_workers=num_workers)\n",
    "        else:\n",
    "            train_loader = DataListLoader(train, batch_size = batch_size, shuffle = True, num_workers=num_workers)\n",
    "            valid_loader = DataListLoader(valid, batch_size = batch_size * valid_batch_size_factor, shuffle = True, num_workers=num_workers)\n",
    "\n",
    "        if False and \"benchmark\":\n",
    "            for batch in tqdm.tqdm_notebook(train_loader):\n",
    "                pass\n",
    "    else:\n",
    "        if to_load:\n",
    "            train_indices = to_load['train_indices']\n",
    "            valid_indices = to_load['valid_indices']\n",
    "        else:\n",
    "            indices = list(range(len(dataset)))\n",
    "            train_indices, valid_indices = train_test_split(indices, test_size = 5000, random_state = 1234)\n",
    "            \n",
    "        train_big_indices, train_small_indices = train_test_split(list(range(len(train_indices))), test_size = 5000, random_state = 1234)\n",
    "\n",
    "        train = torch.utils.data.Subset(dataset, train_indices)\n",
    "        train_small = torch.utils.data.Subset(train, train_small_indices)\n",
    "        valid = torch.utils.data.Subset(dataset, valid_indices)\n",
    "        \n",
    "        if not parallel_gpu:\n",
    "            submit_loader = DataLoader(valid, batch_size = batch_size * valid_batch_size_factor, drop_last = False, shuffle = False, follow_batch=['edge_attr_numeric'], num_workers=num_workers)\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        if False and \"benchmark\":\n",
    "            for batch in tqdm.tqdm_notebook(submit_loader):\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T21:33:40.771141Z",
     "start_time": "2019-08-28T21:33:40.762518Z"
    }
   },
   "outputs": [],
   "source": [
    "def package_valid_prediction(train_predictions):\n",
    "    assert train_predictions.loc[train_predictions['prediction'].notnull()].shape[0] == 792626\n",
    "\n",
    "    train = pd.read_csv('data/train.csv')\n",
    "\n",
    "    train['prediction'] = np.nan\n",
    "\n",
    "    pred_1 = pd.merge(train[['molecule_name', 'atom_index_0', 'atom_index_1']], train_predictions[['molecule_name', 'atom_index_0', 'atom_index_1', 'prediction']], left_on = ['molecule_name', 'atom_index_0', 'atom_index_1'], right_on = ['molecule_name', 'atom_index_0', 'atom_index_1'], how = 'left')['prediction']\n",
    "    pred_2 = pd.merge(train[['molecule_name', 'atom_index_0', 'atom_index_1']], train_predictions[['molecule_name', 'atom_index_0', 'atom_index_1', 'prediction']], left_on = ['molecule_name', 'atom_index_0', 'atom_index_1'], right_on = ['molecule_name', 'atom_index_1', 'atom_index_0'], how = 'left')['prediction']\n",
    "\n",
    "    train['prediction'] = pred_1\n",
    "    train.loc[train['prediction'].isnull(), 'prediction'] = pred_2\n",
    "\n",
    "    assert train['prediction'].isnull().sum() == 4382642\n",
    "    assert train['prediction'].notnull().sum() == 275505\n",
    "\n",
    "    train['dataset'] = 'train'\n",
    "\n",
    "    indices = list(range(len(dataset)))\n",
    "    train_indices, valid_indices = train_test_split(indices, test_size = 5000, random_state = 1234)\n",
    "    valid_molecules = [dataset.molecules[i] for i in valid_indices]\n",
    "\n",
    "    train.loc[train['molecule_name'].isin(valid_molecules), 'dataset'] = 'valid'\n",
    "\n",
    "    assert train.loc[train['dataset'] == 'valid', 'molecule_name'].nunique() == 5000\n",
    "    \n",
    "    train = train.loc[train['prediction'].notnull()]\n",
    "    \n",
    "    return train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T12:18:36.218662Z",
     "start_time": "2019-08-19T12:18:36.216463Z"
    }
   },
   "source": [
    "## classical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T21:33:41.072450Z",
     "start_time": "2019-08-28T21:33:41.070008Z"
    }
   },
   "outputs": [],
   "source": [
    "import layers.layers_09ZB_link_edge\n",
    "import layers.layers_09ZF_ablation_study\n",
    "import layers.layers_09ZF3_ablation_study_remove_global_state\n",
    "import layers.layers_09ZF5_ablation_study_no_edge_pairs_embeddings_and_one_preprocessing\n",
    "import layers.layers_09ZI_distributionnal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T21:33:41.705818Z",
     "start_time": "2019-08-28T21:33:41.697445Z"
    }
   },
   "outputs": [],
   "source": [
    "models_data = [\n",
    "    {\n",
    "        'names' : model_B,\n",
    "        'hidden' : 300,\n",
    "        'layer_count' : 6,\n",
    "        'batch_size' : 20,\n",
    "        'module' : layers.layers_09ZB_link_edge\n",
    "    },\n",
    "    {\n",
    "        'names' : model_F,\n",
    "        'hidden' : 300,\n",
    "        'layer_count' : 6,\n",
    "        'batch_size' : 20,\n",
    "        'module' : layers.layers_09ZF_ablation_study\n",
    "    },\n",
    "    {\n",
    "        'names' : model_F2,\n",
    "        'hidden' : 200,\n",
    "        'layer_count' : 6,\n",
    "        'batch_size' : 64,\n",
    "        'module' : layers.layers_09ZF_ablation_study\n",
    "    },\n",
    "    {\n",
    "        'names' : model_F3,\n",
    "        'hidden' : 200,\n",
    "        'layer_count' : 6,\n",
    "        'batch_size' : 64,\n",
    "        'module' : layers.layers_09ZF3_ablation_study_remove_global_state\n",
    "    },\n",
    "    {\n",
    "        'names' : model_F5,\n",
    "        'hidden' : 200,\n",
    "        'layer_count' : 6,\n",
    "        'batch_size' : 64,\n",
    "        'module' : layers.layers_09ZF5_ablation_study_no_edge_pairs_embeddings_and_one_preprocessing\n",
    "    },\n",
    "    {\n",
    "        'names' : model_G,\n",
    "        'hidden' : 500,\n",
    "        'layer_count' : 10,\n",
    "        'batch_size' : 4,\n",
    "        'module' : layers.layers_09ZB_link_edge\n",
    "    },\n",
    "    {\n",
    "        'names' : model_G2,\n",
    "        'hidden' : 300,\n",
    "        'layer_count' : 10,\n",
    "        'batch_size' : 20,\n",
    "        'module' : layers.layers_09ZB_link_edge\n",
    "    },\n",
    "    {\n",
    "        'names' : model_G4,\n",
    "        'hidden' : 300,\n",
    "        'layer_count' : 6,\n",
    "        'batch_size' : 20,\n",
    "        'module' : layers.layers_09ZB_link_edge\n",
    "    },\n",
    "    {\n",
    "        'names' : model_G5,\n",
    "        'hidden' : 300,\n",
    "        'layer_count' : 6,\n",
    "        'batch_size' : 20,\n",
    "        'module' : layers.layers_09ZB_link_edge\n",
    "    },\n",
    "    {\n",
    "        'names' : model_I,\n",
    "        'hidden' : 300,\n",
    "        'layer_count' : 6,\n",
    "        'batch_size' : 20,\n",
    "        'module' : layers.layers_09ZI_distributionnal_loss\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T19:51:13.236411Z",
     "start_time": "2019-08-28T19:51:13.232220Z"
    }
   },
   "source": [
    "device = 'cuda'\n",
    "bin_count = 260 * 4\n",
    "centers = np.linspace(-40, 220 - 1 / 4, bin_count)\n",
    "delta = (centers[1] - centers[0]) / 2\n",
    "centers += delta\n",
    "delta\n",
    "centers = torch.tensor(centers.reshape(1, -1), dtype = torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T18:28:05.933420Z",
     "start_time": "2019-08-28T18:28:05.928153Z"
    }
   },
   "source": [
    "def batch_submit():\n",
    "    global batch\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # BATCH\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # PREDICT\n",
    "        out = model.forward(\n",
    "            [batch.x_numeric],\n",
    "            batch.x_embeddings,\n",
    "\n",
    "            [batch.edge_attr_numeric], \n",
    "            batch.edge_attr_embeddings,\n",
    "\n",
    "            [batch.u_numeric],\n",
    "            batch.u_embeddings,\n",
    "\n",
    "            batch.edge_index, \n",
    "\n",
    "            batch.batch, \n",
    "            batch.edge_attr_numeric_batch, \n",
    "\n",
    "            batch.y_types, \n",
    "\n",
    "            batch.cycles_edge_index,\n",
    "            batch.cycles_id,\n",
    "\n",
    "            batch.edges_connectivity_ids,\n",
    "            batch.edges_connectivity_features,\n",
    "        )\n",
    "        \n",
    "        out = out * centers\n",
    "            \n",
    "        return out.sum(dim = 1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-28T20:23:29.540933Z",
     "start_time": "2019-08-28T20:06:07.357576Z"
    }
   },
   "outputs": [],
   "source": [
    "num_workers = 7\n",
    "device = 'cuda'\n",
    "parallel_gpu = False\n",
    "\n",
    "# Config\n",
    "\n",
    "for model_data in models_data:\n",
    "    for model_name in model_data['names']:\n",
    "        sub_path = f'submissions/submission_{model_name}.csv'\n",
    "        valid_path = f'submissions/train_{model_name}.csv'\n",
    "        model_file = f'model.{model_name}.bin'\n",
    "\n",
    "        if not os.path.isfile(sub_path):\n",
    "            print(\"predict\", model_name)\n",
    "            hidden = model_data['hidden']\n",
    "            layer_count = model_data['layer_count']\n",
    "            batch_size = model_data['batch_size']\n",
    "            MEGNetList = model_data['module'].MEGNetList\n",
    "\n",
    "            valid_batch_size_factor = 5\n",
    "\n",
    "            if not os.path.isfile(f'model_data/{model_file}'):\n",
    "                os.system(f'aws s3 cp s3://grjhuard-eu-west-1/model_data/{model_file} model_data/{model_file}')\n",
    "            to_load = torch.load(f'model_data/{model_file}', map_location = 'cpu')\n",
    "\n",
    "            # Predict\n",
    "            load_train_dataset()\n",
    "            init_model()\n",
    "            init_valid_dataset()\n",
    "            train_predictions = submit(submit_loader)\n",
    "            train = package_valid_prediction(train_predictions)\n",
    "            train.to_csv(valid_path, index = False)\n",
    "\n",
    "            analyze(train)\n",
    "\n",
    "            load_submit_dataset()\n",
    "            init_model()\n",
    "            init_dataset()\n",
    "            submit_predictions = submit(submit_loader)\n",
    "\n",
    "            test = package_submit_prediction(submit_predictions)\n",
    "            test.to_csv(sub_path, index = False)\n",
    "            \n",
    "            os.system(f'rm model_data/{model_file}')\n",
    "        else:\n",
    "            print(\"already done\", model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "499.261px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
